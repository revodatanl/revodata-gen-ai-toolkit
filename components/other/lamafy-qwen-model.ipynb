{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Qwen to Llama Architecture\n",
    "\n",
    "This conversion allows the Qwen model to run on a serving endpoint as provisiioined throughput.\n",
    "\n",
    "[Source](https://www.databricks.com/blog/serving-qwen-models-databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import huggingface_hub\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "# transformers==4.44.2 is required.\n",
    "from transformers.modeling_utils import (\n",
    "    SAFE_WEIGHTS_INDEX_NAME,\n",
    "    SAFE_WEIGHTS_NAME,\n",
    "    shard_checkpoint,\n",
    ")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B\"\n",
    "dbfs_root_target = \"../fixtures/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Qwen to Llama architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert and save Qwen weights\n",
    "\n",
    "\n",
    "def save_weight(input_dir: str, output_dir: str, shard_size: str = \"2GB\") -> None:\n",
    "    \"\"\"\n",
    "    Copies a Qwen model in the input directory to a Llama-compatible version of it in the output directory.\n",
    "    Injects zeroed-out bias vectors in attention layers where needed in order to make it compatible with the Llama\n",
    "    architecture.  Also updates configuration as needed.\n",
    "    \"\"\"\n",
    "    # Load Qwen state dict from .safetensors files\n",
    "    qwen_state_dict = OrderedDict()\n",
    "    for filepath in os.listdir(input_dir):\n",
    "        if filepath.endswith(\".safetensors\"):\n",
    "            full_path = os.path.join(input_dir, filepath)\n",
    "            with safe_open(full_path, framework=\"pt\", device=\"cpu\") as sf:\n",
    "                for key in sf.keys():\n",
    "                    qwen_state_dict[key] = sf.get_tensor(key)\n",
    "\n",
    "    # Copy tensors and inject bias where needed to match Llama\n",
    "    llama_state_dict = OrderedDict()\n",
    "    for key, value in qwen_state_dict.items():\n",
    "        llama_state_dict[key] = value\n",
    "        # Qwen omits bias on attn.o_proj; Llama expects it\n",
    "        if \"attn.o_proj.weight\" in key:\n",
    "            # Each attn.o_proj.weight needs an associated bias in order to be\n",
    "            # compatible with the Llama architecture. Since Qwen doesn't use this we\n",
    "            # insert zeroed out vectors.\n",
    "            bias_key = key.replace(\"attn.o_proj.weight\", \"attn.o_proj.bias\")\n",
    "            llama_state_dict[bias_key] = torch.zeros_like(value[:, 0]).squeeze()\n",
    "\n",
    "    # Save weights using safetensors\n",
    "    shards, index = shard_checkpoint(\n",
    "        llama_state_dict, max_shard_size=shard_size, weights_name=SAFE_WEIGHTS_NAME\n",
    "    )\n",
    "    for shard_file, shard_data in shards.items():\n",
    "        save_path = os.path.join(output_dir, shard_file)\n",
    "        save_file(shard_data, save_path, metadata={\"format\": \"pt\"})\n",
    "\n",
    "    if index is not None:\n",
    "        with open(\n",
    "            os.path.join(output_dir, SAFE_WEIGHTS_INDEX_NAME), \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            json.dump(index, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_configs(input_dir: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Copies Qwen config and tokenizer files to output_dir, removing Qwen-specific fields\n",
    "    and making them compatible with the Llama architecture.\n",
    "    \"\"\"\n",
    "    config_name = \"config.json\"\n",
    "\n",
    "    # Load Qwen config.json\n",
    "    with open(os.path.join(input_dir, config_name), encoding=\"utf-8\") as f:\n",
    "        qwen_config_dict = json.load(f)\n",
    "\n",
    "    # Modify the Qwen config to look like a Llama model\n",
    "    llama_config_dict = {**qwen_config_dict}\n",
    "    llama_config_dict[\"architectures\"] = [\"LlamaForCausalLM\"]  # now it's Llama 8-)\n",
    "    llama_config_dict[\"model_type\"] = \"llama\"\n",
    "    llama_config_dict[\"attention_bias\"] = True  # Llama-specific\n",
    "    llama_config_dict[\"mlp_bias\"] = False\n",
    "    llama_config_dict[\"pretraining_tp\"] = 0\n",
    "\n",
    "    # Remove Qwen-specific fields related to sliding window\n",
    "    for del_key in [\"sliding_window\", \"use_sliding_window\", \"max_window_layers\"]:\n",
    "        if del_key in llama_config_dict:\n",
    "            del llama_config_dict[del_key]\n",
    "\n",
    "    # Write updated config to the new directory\n",
    "    with open(os.path.join(output_dir, config_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(llama_config_dict, f, indent=2)\n",
    "\n",
    "    # Copy other relevant files (tokenizer, merges, vocab, and so on)\n",
    "    additional_files = [\n",
    "        \"generation_config.json\",\n",
    "        \"merges.txt\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"vocab.json\",\n",
    "    ]\n",
    "    for fname in additional_files:\n",
    "        src = os.path.join(input_dir, fname)\n",
    "        dst = os.path.join(output_dir, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Function to Orchestrate the Qwen→Llama Conversion\n",
    "\n",
    "\n",
    "def llamafy_qwen(input_dir: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts Qwen2.5 into a Llama-like architecture by rewriting weights and configs.\n",
    "    After this step, the resulting folder can be treated as if it's a Llama model.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "\n",
    "    # Rewrite Qwen weights to add missing biases\n",
    "    save_weight(input_dir, output_dir)\n",
    "\n",
    "    # Update config to make it a Llama model and copy other files\n",
    "    save_configs(input_dir, output_dir)\n",
    "\n",
    "    print(\n",
    "        f\"Successfully converted Qwen from '{input_dir}' to Llama format at '{output_dir}'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_name\n",
    "assert dbfs_root_target\n",
    "\n",
    "target_dbfs_path = os.path.join(dbfs_root_target, model_name)\n",
    "target_dbfs_modified_path = f\"{target_dbfs_path}-Llama\"\n",
    "\n",
    "if not os.path.exists(Path(target_dbfs_path).parent):\n",
    "    os.makedirs(Path(target_dbfs_path).parent)\n",
    "\n",
    "if not os.path.exists(target_dbfs_path):\n",
    "    print(f\"Downloading to {target_dbfs_path}\")\n",
    "    huggingface_hub.snapshot_download(model_name, local_dir=target_dbfs_path)\n",
    "else:\n",
    "    print(f\"Already exists: {target_dbfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(target_dbfs_modified_path):\n",
    "    print(f\"Modifying Qwen model and writing to {target_dbfs_modified_path}\")\n",
    "    llamafy_qwen(target_dbfs_path, target_dbfs_modified_path)\n",
    "else:\n",
    "    print(f\"Already exists: {target_dbfs_modified_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Lamafied Qwen Model to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/LamafiedQwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"../../fixtures/models/Qwen/Qwen2.5-Coder-7B-Llama\"\n",
    "catalog = \"gen_ai_toolkit\"\n",
    "schema = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path\n",
    "\n",
    "if not model_name:\n",
    "    model_name = model_path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    model_name = model_name.replace(\".\", \"\").lower()\n",
    "\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the tokenizer files and model config in the source directory\n",
    "\n",
    "# Load the tokenizer config\n",
    "with open(os.path.join(model_path, \"tokenizer_config.json\")) as f:\n",
    "    tokenizer_config_obj = json.loads(f.read())\n",
    "\n",
    "# Remove Qwen’s chat template since it is not recognized by model serving.\n",
    "if \"chat_template\" in tokenizer_config_obj:\n",
    "    del tokenizer_config_obj[\"chat_template\"]\n",
    "\n",
    "# Update the tokenizer class so that Databricks sees it as \"PreTrainedTokenizerFast\",\n",
    "# since Qwen's tokenizer is not recognized, and it derives from PreTrainedTokenizerFast.\n",
    "# This also avoids saving additional files during registration that model\n",
    "# serving would not expect.\n",
    "tokenizer_config_obj[\"tokenizer_class\"] = \"PreTrainedTokenizerFast\"\n",
    "\n",
    "# Model serving expects model_input_names to be specified for Llama models.\n",
    "tokenizer_config_obj[\"model_input_names\"] = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "# Write the updated configs back\n",
    "with open(os.path.join(model_path, \"tokenizer_config.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(tokenizer_config_obj, indent=2))\n",
    "\n",
    "config_path = os.path.join(model_path, \"config.json\")\n",
    "with open(config_path) as f:\n",
    "    config_obj = json.loads(f.read())\n",
    "\n",
    "# Set 'max_position_embeddings' to 16000 for compatibility with certain throughput settings.\n",
    "config_obj[\"max_position_embeddings\"] = 16000\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(json.dumps(config_obj, indent=2))\n",
    "\n",
    "# don't need the slow tokenizer files\n",
    "files_to_delete = [\"merges.txt\", \"vocab.json\"]\n",
    "for file_to_delete in files_to_delete:\n",
    "    if os.path.exists(os.path.join(model_path, file_to_delete)):\n",
    "        os.remove(os.path.join(model_path, file_to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model as a Llama model\n",
    "\n",
    "# We'll create metadata that references Llama so Databricks sees \"LlamaForCausalLM\".\n",
    "# Some of the specific versions/sizes referenced here don't matter for our purposes.\n",
    "# The important thing is that model serving considers this a Llama model.\n",
    "# Note that we register this as a completion model and removed the chat template.\n",
    "# Chat formatting will need to be performed by the client.\n",
    "task = \"llm/v1/completions\"\n",
    "metadata = {\n",
    "    \"task\": task,\n",
    "    \"curation_version\": 1,\n",
    "    \"databricks_model_family\": \"LlamaForCausalLM (llama-3.2)\",\n",
    "    \"databricks_model_size_parameters\": \"7b\",\n",
    "    \"databricks_model_source\": \"genai-fine-tuning\",\n",
    "    \"source\": \"huggingface\",\n",
    "    \"source_model_name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"source_model_revision\": \"0cb88a4f764b7a12671c53f0838cd831a0843b95\",\n",
    "}\n",
    "\n",
    "input_example = {\n",
    "    \"prompt\": \"def print_hello_world():\",\n",
    "    \"max_tokens\": 20,\n",
    "    \"temperature\": 0.05,\n",
    "    \"stop\": [\"\\n\\n\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert catalog\n",
    "assert schema\n",
    "assert model_name\n",
    "\n",
    "registered_model_name = \".\".join([catalog, schema, model_name])\n",
    "\n",
    "print(f\"Registering model as {registered_model_name}\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model=model_path,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=registered_model_name,\n",
    "        input_example=input_example,\n",
    "        metadata=metadata,\n",
    "        task=task,\n",
    "        torch_dtype=\"bfloat16\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
