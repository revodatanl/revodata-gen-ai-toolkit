{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to MLflow in Databricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/ConversationalRAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model as code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UC_CATALOG_NAME = \"generative_ai_toolkit.use_cases.conversational_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is Retrieval-augmented Generation?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=\"ConversationalRAG\",\n",
    "    tags={\"ml_type\": \"genai\", \"architecture\": \"rag\"},\n",
    "    description=\"Conversational RAG API using the ChatAgent class\",\n",
    "):\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        streamable=True,\n",
    "        pip_requirements=\"../../requirements.txt\",\n",
    "        python_model=\"ConversationalRAG.py\",\n",
    "        registered_model_name=UC_CATALOG_NAME,\n",
    "        input_example=input_example,\n",
    "        example_no_conversion=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"MLflow Run: {model_info.run_id}\")\n",
    "print(f\"Model URI: {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get all versions of the model\n",
    "model_versions = client.search_model_versions(f\"name='{UC_CATALOG_NAME}'\")\n",
    "\n",
    "# Get the latest version number\n",
    "latest_version = max([int(mv.version) for mv in model_versions])\n",
    "\n",
    "print(f\"The latest version of model '{UC_CATALOG_NAME}' is: {latest_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import os\n",
    "\n",
    "# In the UI you need to change the deployment to a GPU instance, this can not be configured in the code\n",
    "# This is optional, you can also test your modal locally as shown in the next cell\n",
    "deployment = agents.deploy(\n",
    "    model_name=UC_CATALOG_NAME,\n",
    "    model_version=latest_version,\n",
    "    scale_to_zero=True,\n",
    "    environment_vars={\n",
    "        \"DATABRICKS_HOST\": os.getenv(\"DATABRICKS_HOST\"),\n",
    "        \"DATABRICKS_CLIENT_ID\": os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "        \"DATABRICKS_CLIENT_SECRET\": os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "    },\n",
    "    endpoint_name=\"conversational_rag_endpoint\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally test the registered model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = my_model.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the medallion arthicecture?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in my_model.predict_stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about data engineering, specifically about the delta lake definitive guide which is book you have access to. You will recieve the user question and number of extracts from the book that you should use to answer the question. If you can not answer the question based on the information provided, just say so. Do not make up information. Never mention that you have recieved extracts from the book.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How did ETL work in the first generation platforms?\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(token.get(\"delta\").get(\"content\"), flush=True, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
