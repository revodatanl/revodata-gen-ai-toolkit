{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Chunking\n",
    "\n",
    "This notebook is a work in progress and that shows how to perform late chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"../../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# We will be using docling to conver the PDF into markdown.\n",
    "\n",
    "# This might run for a couple minutes as the PDF is fairly large.\n",
    "source = \"../../fixtures/Delta Lake Definitive Guide.pdf\"\n",
    "converter = DocumentConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = converter.convert(source)\n",
    "\n",
    "document = result.document.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "\n",
    "# for this model you need to define the embeddings task; which can not be done with the late chunking function\n",
    "# It will use a non lora adapter; so just a regular embedding instead of query of document specialisation\n",
    "# MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "jina_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "jina_model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(document, model, tokenizer):\n",
    "    \"Implements late chunking on a document.\"\n",
    "\n",
    "    # Tokenize with offset mapping to find sentence boundaries\n",
    "    inputs_with_offsets = tokenizer(\n",
    "        document, return_tensors=\"pt\", return_offsets_mapping=True\n",
    "    )\n",
    "    token_offsets = inputs_with_offsets[\"offset_mapping\"][0]\n",
    "    token_ids = inputs_with_offsets[\"input_ids\"][0]\n",
    "\n",
    "    # Find chunk boundaries\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids(\".\")\n",
    "    chunk_positions, token_span_annotations = [], []\n",
    "    span_start_char, span_start_token = 0, 0\n",
    "\n",
    "    for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\n",
    "        if i < len(token_ids) - 1:\n",
    "            if token_id == punctuation_mark_id and document[end : end + 1] in [\n",
    "                \" \",\n",
    "                \"\\n\",\n",
    "            ]:\n",
    "                # Store both character positions and token positions\n",
    "                chunk_positions.append((span_start_char, int(end)))\n",
    "                token_span_annotations.append((span_start_token, i + 1))\n",
    "\n",
    "                # Update start positions for next chunk\n",
    "                span_start_char, span_start_token = int(end) + 1, i + 1\n",
    "\n",
    "    # Create text chunks from character positions\n",
    "    chunks = [document[start:end].strip() for start, end in chunk_positions]\n",
    "\n",
    "    # Encode the entire document\n",
    "    inputs = tokenizer(document, return_tensors=\"pt\")\n",
    "    model_output = model(**inputs)\n",
    "    token_embeddings = model_output[0]\n",
    "\n",
    "    # Create embeddings for each chunk using mean pooling\n",
    "    embeddings = []\n",
    "    for start_token, end_token in token_span_annotations:\n",
    "        if end_token > start_token:  # Ensure span has at least one token\n",
    "            chunk_embedding = token_embeddings[0, start_token:end_token].mean(dim=0)\n",
    "            embeddings.append(chunk_embedding.detach().cpu().numpy())\n",
    "\n",
    "    return chunks, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Late chunking the full document is too memory intensive, so we'll split the document into chunks by header.\n",
    "# These are then further chunked using late chunking\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    # (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on,\n",
    "    # return_each_line=True,\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(document)\n",
    "\n",
    "print(f\"Number of chunks: {len(md_header_splits)}\")\n",
    "\n",
    "for document in md_header_splits:\n",
    "    tokens = (\n",
    "        jina_tokenizer(document.page_content, return_tensors=\"pt\")\n",
    "        .get(\"input_ids\")\n",
    "        .shape[1]\n",
    "    )\n",
    "    if tokens <= 32000:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"too many tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_chunks = []\n",
    "late_embeddings = []\n",
    "\n",
    "for document in md_header_splits:\n",
    "    tmp_late_chunks, tmp_late_embeddings = late_chunking(\n",
    "        document.page_content, jina_model, jina_tokenizer\n",
    "    )\n",
    "    late_chunks.extend(tmp_late_chunks)\n",
    "    late_embeddings.extend(tmp_late_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
