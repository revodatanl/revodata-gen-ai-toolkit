{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the PDF and upload to the Vector Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# We will be using docling to conver the PDF into markdown.\n",
    "\n",
    "# This might run for a couple minutes as the PDF is fairly large.\n",
    "source = \"../../fixtures/Delta Lake Definitive Guide.pdf\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk the parsed pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Docling has a hybrid chunker that uses a recursive approach to chunk the document.\n",
    "\n",
    "# We can set a max number of tokens per chunk so that the chunks fit in the context windows of the embedding model.\n",
    "# The \"jinaai/jina-embeddings-v3\" has a context window of 8192 tokens.\n",
    "# However, the reranker model used in the upcoming modules has token window of 1024\n",
    "# While theoretically the rerank uses a slidign widow approach for chunks longer than 1024, we simply enforoce a max token limit per chunk of 1024.\n",
    "\n",
    "# We get the embedding model from huggingface\n",
    "EMBED_MODEL_ID = \"jinaai/jina-embeddings-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)\n",
    "\n",
    "\n",
    "# We set the max tokens to 8192 / 8 = 1024\n",
    "MAX_TOKENS = 8192 / 8\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    merge_peers=True,\n",
    ")\n",
    "\n",
    "chunk_iter = chunker.chunk(dl_doc=result.document)\n",
    "chunks = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structure and Embed the chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Here we use the \"jinaai/jina-embeddings-v3\" model to embed the chunk text.\n",
    "model_passage = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v3\",\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"default_task\": \"retrieval.passage\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using 'direct access' to the vector index. Therefor we need to provide the text and an embedding vector.\n",
    "# You will also be able to create a vector index that automatically creates the embeddings vector.\n",
    "# However you need to have an embeddings enpoints with task `llm/v1/embeddings` - which you can host with a serving endpoint.\n",
    "# But as we do not have this currently, we will manually create the embeddings vector.\n",
    "\n",
    "simple_chunks = []\n",
    "for ix, chunk in enumerate(chunks):\n",
    "    chunk_json = chunk.export_json_dict()\n",
    "\n",
    "    # When creating the vector index we will define the following schema.\n",
    "    # Note that a unique index is required, if you rerun this code with new data ix will be different as it start from 0 again.\n",
    "    # Consider adding a distinct, consistent id for each chunk.\n",
    "    simple_chunk = {\n",
    "        \"id\": ix,\n",
    "        \"text\": str(chunk_json[\"text\"]),\n",
    "        \"text_vector\": model_passage.encode(chunk_json[\"text\"]).tolist(),\n",
    "        \"heading\": str(chunk_json.get(\"meta\", {}).get(\"headings\", \"\") or \"\"),\n",
    "    }\n",
    "\n",
    "    simple_chunks.append(simple_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "simple_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Insert the chunks to the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://docs.databricks.com/aws/en/generative-ai/create-query-vector-search\n",
    "\n",
    "# Define the catalog, schema, vector search endpoint name, and vector index name\n",
    "CATALOG = \"generative_ai_toolkit\"\n",
    "SCHEMA = \"use_cases\"\n",
    "vector_search_endpoint_name = \"generative_ai_toolkit_vs_endpoint\"\n",
    "INDEX = \"delta_lake_definitive_guide_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession as SparkSession\n",
    "\n",
    "spark = SparkSession.builder.remote(serverless=True).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Initialise the vector search client\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "\n",
    "# Check if the endpoint exists\n",
    "def endpoint_exists(endpoint_name: str, client: VectorSearchClient):\n",
    "    try:\n",
    "        return client.get_endpoint(endpoint_name)\n",
    "    except Exception as e:\n",
    "        if \"NOT_FOUND\" in str(e):\n",
    "            return False\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "# Create the vector search (store) endpoint\n",
    "def create_vector_search_endpoint(endpoint_name: str, client: VectorSearchClient):\n",
    "    endpoint = endpoint_exists(endpoint_name, client)\n",
    "    if not endpoint:\n",
    "        try:\n",
    "            vsc.create_endpoint_and_wait(\n",
    "                name=endpoint_name, endpoint_type=\"STANDARD\", verbose=True\n",
    "            )\n",
    "            return client.get_endpoint(endpoint_name)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    elif endpoint[\"endpoint_status\"][\"state\"] == \"ONLINE\":\n",
    "        return endpoint\n",
    "    elif endpoint[\"endpoint_status\"][\"state\"] == \"PROVISIONING\":\n",
    "        raise Exception(f\"Endpoint is provisioning - {endpoint}\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Error with the endpoint - this shouldn't happen. Please delete it and try again.\\nEndpoint details: {endpoint}\"\n",
    "        )\n",
    "\n",
    "\n",
    "create_vector_search_endpoint(vector_search_endpoint_name, client=vsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index exists\n",
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        index = vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            return False\n",
    "\n",
    "\n",
    "# Create the index if it does not exist\n",
    "if not index_exists(vsc, vector_search_endpoint_name, f\"{CATALOG}.{SCHEMA}.{INDEX}\"):\n",
    "    try:\n",
    "        index = vsc.create_direct_access_index(\n",
    "            endpoint_name=vector_search_endpoint_name,\n",
    "            index_name=f\"{CATALOG}.{SCHEMA}.{INDEX}\",\n",
    "            primary_key=\"id\",\n",
    "            embedding_dimension=1024,\n",
    "            embedding_vector_column=\"text_vector\",\n",
    "            schema={\n",
    "                \"id\": \"int\",\n",
    "                \"text\": \"string\",\n",
    "                \"text_vector\": \"array<float>\",\n",
    "                \"heading\": \"string\",\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    else:\n",
    "        index = vsc.get_index(\n",
    "            vector_search_endpoint_name, f\"{CATALOG}.{SCHEMA}.{INDEX}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert the chunks into the index\n",
    "index.upsert(simple_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the vector index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References: https://api-docs.databricks.com/python/vector-search/databricks.vector_search.html#databricks.vector_search.index.VectorSearchIndex.upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did ETL work in the first generation platforms?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same embedding model to embed the query.\n",
    "model_passage = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v3\",\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"default_task\": \"retrieval.query\"},  # this time be embed for query\n",
    ")\n",
    "\n",
    "query_vector = model_passage.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide both the query text and the query vector for the similarity search\n",
    "results = index.similarity_search(\n",
    "    query_text=query,\n",
    "    query_vector=query_vector,\n",
    "    columns=[\"id\", \"text\", \"heading\"],\n",
    "    num_results=8,\n",
    "    query_type=\"hybrid\",\n",
    "    # filters={\"heading NOT\": '1 Introduction'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply structure the retrieved results\n",
    "\n",
    "\n",
    "def parse_results(results):\n",
    "    # Extract the data from the results\n",
    "    data_array = results[\"result\"][\"data_array\"]\n",
    "    columns = results[\"manifest\"][\"columns\"]\n",
    "\n",
    "    # Create a list of dictionaries for each result\n",
    "    parsed_results = []\n",
    "    for item in data_array:\n",
    "        parsed_result = {}\n",
    "        for i, column in enumerate(columns):\n",
    "            column_name = column[\"name\"]\n",
    "            parsed_result[column_name] = item[i]\n",
    "        parsed_results.append(parsed_result)\n",
    "\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "parsed_results = parse_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
