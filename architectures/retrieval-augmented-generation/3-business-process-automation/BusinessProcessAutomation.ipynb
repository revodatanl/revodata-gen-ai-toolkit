{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to MLflow in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/BusinessAutomation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "model_input_example = \"How did ETL work in the first generation platforms?\"\n",
    "\n",
    "model_output_example = {\n",
    "    \"question\": \"How did ETL work in the first generation platforms?\",\n",
    "    \"answer\": \"In first generation data platforms, ETL (Extract, Transform, Load) processes were typically batch-oriented and relied heavily on manual coding. Data was extracted from source systems, transformed using custom scripts or specialized ETL tools, and then loaded into data warehouses. These processes were often rigid, time-consuming, and required significant maintenance overhead.\",\n",
    "    \"context\": [\n",
    "        {\n",
    "            \"corpus_id\": \"doc123\",\n",
    "            \"id\": \"section_5.2\",\n",
    "            \"text\": \"First generation data platforms relied on batch ETL processes that extracted data from operational systems during off-hours, transformed it using custom code, and loaded it into data warehouses for analysis.\",\n",
    "            \"score\": 0.95,\n",
    "        },\n",
    "        {\n",
    "            \"corpus_id\": \"doc456\",\n",
    "            \"id\": \"section_3.7\",\n",
    "            \"text\": \"Traditional ETL workflows in early data platforms were characterized by rigid schedules, limited scalability, and high maintenance costs compared to modern approaches.\",\n",
    "            \"score\": 0.87,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "signature = infer_signature(model_input_example, model_output_example)\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        streamable=True,\n",
    "        pip_requirements=\"../../requirements.txt\",\n",
    "        python_model=\"BusinessAutomation.py\",\n",
    "        signature=signature,\n",
    "        input_example=model_input_example,\n",
    "    )\n",
    "\n",
    "my_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.predict([\"How did ETL work in the first generation platforms?\", \"hi\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
