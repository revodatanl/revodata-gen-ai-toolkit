{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to MLflow in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/ChatAgentLangGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        code_paths=None,  # location of the dependencies, this way you do not need to build a wheel file.\n",
    "        streamable=True,\n",
    "        pip_requirements=\"../../requirements.txt\",\n",
    "        python_model=\"ChatAgentLangGraph.py\",\n",
    "        input_example={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "        },\n",
    "    )\n",
    "\n",
    "my_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Calculate 9 + 1 with your available tools\"}\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_model.predict_stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 12 + 8?\"}]}\n",
    "):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from scratch.LangGraph import agent\n",
    "\n",
    "# Display the agent's graph structure using Mermaid\n",
    "if hasattr(agent, \"get_graph\"):\n",
    "    graph = agent.get_graph()\n",
    "    try:\n",
    "        # Use the Mermaid visualization method with PNG output\n",
    "        display(\n",
    "            Image(\n",
    "                graph.draw_mermaid_png(\n",
    "                    draw_method=MermaidDrawMethod.API,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        print(\"Graph structure:\", graph)\n",
    "else:\n",
    "    print(\"Agent doesn't have a graph visualization method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Mimick a frontend by parsing the respones from the agent differently depending on the message type.\n",
    "\n",
    "\n",
    "def parse_and_display_api_response(chunk):\n",
    "    \"\"\"\n",
    "    Parse and display a single chunk from a streaming LLM API response.\n",
    "\n",
    "    Args:\n",
    "        chunk: A single chunk from the streaming API response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if this is a tuple containing messages\n",
    "        if isinstance(chunk, tuple) and len(chunk) >= 2 and chunk[0] == \"messages\":\n",
    "            # Access the message chunk (AIMessageChunk) and metadata\n",
    "            message_chunk = chunk[1][0]  # First element of the second tuple item\n",
    "\n",
    "            # Handle tool calls\n",
    "            if (\n",
    "                hasattr(message_chunk, \"additional_kwargs\")\n",
    "                and \"tool_calls\" in message_chunk.additional_kwargs\n",
    "            ):\n",
    "                tool_calls = message_chunk.additional_kwargs.get(\"tool_calls\", [])\n",
    "\n",
    "                # Display tool calls\n",
    "                for tool_call in tool_calls:\n",
    "                    # Extract function name and arguments\n",
    "                    function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n",
    "                    if not function_name and \"id\" in tool_call:\n",
    "                        # Try to get name from id if available\n",
    "                        function_name = (\n",
    "                            tool_call.get(\"id\", \"\").split(\"_\")[1]\n",
    "                            if \"_\" in tool_call.get(\"id\", \"\")\n",
    "                            else \"\"\n",
    "                        )\n",
    "\n",
    "                    arguments = tool_call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "\n",
    "                    # Only display if we have meaningful data\n",
    "                    if function_name or arguments:\n",
    "                        print(\"\\033[33m\", end=\"\")  # Yellow color for tool calls\n",
    "                        print(f\"TOOL CALL: {function_name}\")\n",
    "\n",
    "                        # Format arguments if they're JSON\n",
    "                        if arguments and arguments.strip():\n",
    "                            try:\n",
    "                                arg_obj = json.loads(arguments)\n",
    "                                args_formatted = json.dumps(arg_obj, indent=2)\n",
    "                                print(f\"ARGUMENTS: {args_formatted}\")\n",
    "                            except Exception as e:\n",
    "                                if arguments:\n",
    "                                    print(f\"ARGUMENTS: {arguments}\")\n",
    "                                raise e\n",
    "                        print(\"\\033[0m\", end=\"\")  # Reset color\n",
    "\n",
    "            # Display content if present\n",
    "            if hasattr(message_chunk, \"content\") and message_chunk.content:\n",
    "                # Print content in normal color (no tool call)\n",
    "                print(message_chunk.content, end=\"\", flush=True)\n",
    "\n",
    "            # Display finish reason if present\n",
    "            if hasattr(\n",
    "                message_chunk, \"response_metadata\"\n",
    "            ) and message_chunk.response_metadata.get(\"finish_reason\"):\n",
    "                finish_reason = message_chunk.response_metadata.get(\"finish_reason\")\n",
    "                print(f\"\\n\\033[32m[FINISHED: {finish_reason}]\\033[0m\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\033[31mError parsing chunk: {e}\\033[0m\")\n",
    "        print(f\"Raw chunk: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is 1000000 + 4, break it down in details, us the function then explain the answer.\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=[\"messages\"],\n",
    "):\n",
    "    parse_and_display_api_response(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
