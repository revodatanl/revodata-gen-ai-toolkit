{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/ChatAgentTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        # code_paths=None,  # location of the dependencies, this way you do not need to build a wheel file.\n",
    "        # registered_model_name=\"ai_recruiter.candidates.agent_prototype\",\n",
    "        streamable=True,\n",
    "        pip_requirements=\"../requirements.txt\",\n",
    "        python_model=\"ChatAgentTools.py\",\n",
    "        # prompts=None,  # a list of prompts registered in the prompt registry; prompt:/<name>/<version> - not supported in databricks yet\n",
    "        metadata={\n",
    "            \"max_tokens\": 8000,\n",
    "            \"model_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        },\n",
    "        input_example={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "        },\n",
    "    )\n",
    "\n",
    "my_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator from predict_stream\n",
    "stream_generator = my_model.predict_stream(\n",
    "    data={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is 12 + 8?\"}],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Iterate through the generator to get responses\n",
    "for response in stream_generator:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_model.predict(\n",
    "    data={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is 12 + 8?\"}],\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ipynb-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
