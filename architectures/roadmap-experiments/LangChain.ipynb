{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The login and mlflow model registery are set this way because of local development. If you are running this in Databricks, you can remove the login and use the regualr mlflow registry.\n",
    "\n",
    "mlflow.login()\n",
    "\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"/LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "example_input = {\"topic\": \"AI\"}\n",
    "example_output = {\n",
    "    \"content\": \"Why did the AI program go on a diet?\\n\\nBecause it wanted to lose some bytes! (get it?)\",\n",
    "    \"additional_kwargs\": {},\n",
    "    \"response_metadata\": {\n",
    "        \"prompt_tokens\": 17,\n",
    "        \"completion_tokens\": 23,\n",
    "        \"total_tokens\": 40,\n",
    "    },\n",
    "    \"type\": \"ai\",\n",
    "    \"name\": None,\n",
    "    \"id\": \"run-7c12efc1-f2bb-459d-8bfd-c8bba357bf7f-0\",\n",
    "    \"example\": False,\n",
    "    \"tool_calls\": [],\n",
    "    \"invalid_tool_calls\": [],\n",
    "    \"usage_metadata\": None,\n",
    "}\n",
    "\n",
    "signature = infer_signature(example_input, example_output)\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        artifact_path=\"tech_chain\",\n",
    "        signature=signature,\n",
    "        lc_model=\"LangChain.py\",\n",
    "        # registered_model_name=\"ai_recruiter.candidates.agent_prototype\",\n",
    "        streamable=True,\n",
    "        pip_requirements=\"../requirements.txt\",\n",
    "    )\n",
    "\n",
    "my_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_model.predict_stream({\"topic\": \"AI\"}):\n",
    "    print(i.get(\"content\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.predict({\"topic\": \"AI\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
