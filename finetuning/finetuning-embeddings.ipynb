{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4079e9aa-e3f8-4ad6-a2a9-ec5c8cbc44df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Finetune an Embeddings Model using Databricks GPU Serverless\n",
    "\n",
    "Databricks now supports serverless compute accelerated with graphics processing units (GPUs), which can be used to train or fine-tune custom models using the framework of your choice while achieving state-of-the-art efficiency, performance, and quality. This notebook describes how you can use GPU-accelerated Serverless to finetune an embeddings model on your own data using the Sentence Transformer package.\n",
    "\n",
    "We were inspired by Phil Schmid's (Google DeepMind) article [Fine-tune Embedding models for Retrieval Augmented Generation](https://www.philschmid.de/fine-tune-embedding-model-for-rag) and have adapted the code to run on Databricks, utilizing the [MLflow Sentence Transformer Flavor](https://mlflow.org/docs/latest/ml/deep-learning/sentence-transformers/guide).\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "Using the sentence-transformers package, we will finetune the `modernbert-embed-base` model on our own data. Finetuning an embeddings model can yield significant improvements in the retrieval step of a RAG application. Our knowledge base consists of roughly 40 scientific papers covering various GenAI topics.\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "1. Load the PDFs (which are stored in an external volume) into a Delta table using Auto loader with the Binary file type.\n",
    "\n",
    "2. Convert the binary file content into markdown text using `pymupdf4llm`.\n",
    "\n",
    "3. Chunk the markdown text into smaller pieces (standard practice for RAG applications).\n",
    "\n",
    "4. Generate a synthetic question for each chunk.\n",
    "\n",
    "   For our loss function we will use the `MultipleNegativesRankingLoss`, which expects a dataset structured as the example below. However, we do not have an 'anchor' or question yet.\n",
    "\n",
    "   ```json\n",
    "   [\n",
    "     {\n",
    "       \"anchor\": \"what is rag?\",\n",
    "       \"positive\": \"rag stands for retrieval augmented generation and is a method to ...\",\n",
    "       \"global_chunk_id\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\"\n",
    "     }\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "5. Evaluate the retrieval performance on the `modernbert-embed-base`\n",
    "6. Finetune the model using our chunks and synthetic questions\n",
    "7. Evaluate the retrieval performance on our new model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150fd197-1fbb-43b6-b13e-36bcd15be156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "Before getting started, you need to connect to an GPU accelerated Serverless instance.\n",
    "\n",
    "1. Navigate to the Environment side panel on the rightmost side of the notebook.\n",
    "2. Set Accelerator to A10G for this demo.\n",
    "3. You do not need to install any dependencies in the environment panel.\n",
    "4. Select 3 as your Environment version\n",
    "5. Select Apply and then Confirm you want to apply this environment to your notebook.\n",
    "\n",
    "Now we can start!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14dfbd53-e323-4cb6-ad8c-c87561d8361b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the required packges\n",
    "%pip install pymupdf>=1.26.0 pymupdf4llm>=0.0.24 langchain-text-splitters sentence-transformers==4.1.0 transformers[torch] datasets markdownify>=1.1.0 plotly\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293484ea-c9ed-4070-8e17-a5df3b3b4451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the variables used across the notebook (change these to your own values)\n",
    "\n",
    "CATALOG = \"justin_zweep_gen_ai_demos\"\n",
    "SCHEMA = \"embeddings\"\n",
    "\n",
    "volume_name = \"papers\"\n",
    "pdf_volume = \"/Volumes/justin_zweep_gen_ai_demos/embeddings/papers\"\n",
    "\n",
    "bronze_table_name = \"papers_bronze\"\n",
    "silver_table_name = \"papers_silver\"\n",
    "\n",
    "chunks_table_name = \"paper_chunks\"\n",
    "chunks_with_questions_table_name = \"paper_chunks_w_questions\"\n",
    "\n",
    "# https://huggingface.co/nomic-ai/modernbert-embed-base\n",
    "model_id = \"nomic-ai/modernbert-embed-base\"\n",
    "finetune_model_id = \"modernbert-embed-base-finetuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72859653-943f-4b54-b352-751da68d452a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create objects in unity catalog\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# After creating the folume you need to upload your PDF's here.\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{volume_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8d670c-d994-4e82-a01c-1c84575ad302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load the PDFs into a Delta table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0096932b-be66-486e-879f-b1e171e58593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, current_user\n",
    "\n",
    "(\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"binaryFile\")  # we read the PDF file content as binary\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\")  # ensure we only load PDF files\n",
    "    .load(pdf_volume)\n",
    "    .select(\n",
    "        col(\"content\").alias(\"file_content\"),\n",
    "        col(\"path\").alias(\"file_path\"),\n",
    "        col(\"modificationTime\").alias(\"modification_time\"),\n",
    "        col(\"length\").alias(\"file_size_bytes\"),\n",
    "        current_timestamp().alias(\"_load_timestamp\"),\n",
    "        current_user().alias(\"_load_user\"),\n",
    "    )\n",
    "    .writeStream.format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\n",
    "        \"checkpointLocation\", f\"{pdf_volume}/_checkpoints_2\"\n",
    "    )  # set a checkpoint to ingeset files only once\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{CATALOG}.{SCHEMA}.{bronze_table_name}\")\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21461a55-b246-40c4-874d-a412c2f57f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Convert the binary file content into markdown text\n",
    "\n",
    "First we define a UDF that converts the binary data to markdown, then we apply this to the bronze table and store it in a silver table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56dcb2e-a629-4ba9-ae0b-46192f4a174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "\n",
    "import fitz\n",
    "import pymupdf4llm\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "\n",
    "@udf(\n",
    "    returnType=StructType(\n",
    "        [\n",
    "            StructField(\"content\", StringType(), True),\n",
    "            StructField(\"parser_status\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "def parse_file(content_bytes):\n",
    "    try:\n",
    "        pdf_doc = fitz.Document(stream=content_bytes, filetype=\"pdf\")\n",
    "        md_text = pymupdf4llm.to_markdown(pdf_doc)\n",
    "\n",
    "        parsed_document = {\n",
    "            \"content\": md_text.strip(),\n",
    "            \"parser_status\": \"SUCCESS\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "        warnings.warn(status)\n",
    "        parsed_document = {\n",
    "            \"content\": \"\",\n",
    "            \"parser_status\": f\"ERROR: {status}\",\n",
    "        }\n",
    "    return parsed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e728f8c3-2beb-48a3-96e8-97f55be7ff86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import reverse, split\n",
    "\n",
    "(\n",
    "    spark.read.table(f\"{CATALOG}.{SCHEMA}.{bronze_table_name}\")\n",
    "    .withColumn(\"markdown_content\", parse_file(col(\"file_content\")))\n",
    "    .withColumn(\"file_name\", reverse(split(\"file_path\", \"/\"))[0])\n",
    "    .select(\"file_path\", \"markdown_content.*\", \"file_name\")\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{silver_table_name}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4bf998-9a45-4693-9200-7057e461f3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{silver_table_name}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad08fe0b-4a6a-4cf4-9b44-296faa32a165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Chunk the markdown text\n",
    "\n",
    "First we define a UDF that chunks the markdown text, next we apply it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b196b1-1704-4de5-815a-44f0a6f84840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "def create_markdown_chunker(max_sequence_length: int):\n",
    "    chunk_schema = ArrayType(StringType())\n",
    "\n",
    "    @udf(returnType=chunk_schema)\n",
    "    def _chunk_markdown(markdown_text):\n",
    "        if not markdown_text or markdown_text.strip() == \"\":\n",
    "            return []\n",
    "        try:\n",
    "            from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "            from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "            headers_to_split_on = [\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "                (\"####\", \"Header 4\"),\n",
    "            ]\n",
    "            markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "                headers_to_split_on, strip_headers=False\n",
    "            )\n",
    "\n",
    "            # While a token is not equal to a character, we apply as if it is.\n",
    "            # The token splitter does not work as serverlesss is read only file system.\n",
    "            # And it needs to downlad the tokenizer in order to work.\n",
    "            md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "            chunk_size = max_sequence_length * 0.9\n",
    "            chunk_overlap = max_sequence_length * 0.1\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "            )\n",
    "\n",
    "            txt_splits = text_splitter.split_documents(md_header_splits)\n",
    "            splits = [split.page_content for split in txt_splits]\n",
    "\n",
    "            return splits\n",
    "\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "    return _chunk_markdown\n",
    "\n",
    "\n",
    "# The modern bart model can handle a sequence length of 8912, in this case we ensure chunks are no longer then roughly a quarter of the sequence length. We do this so we have a more (smaller) chunks to finetune on.\n",
    "markdown_chunker = create_markdown_chunker(max_sequence_length=(8912 * 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbd06cb-9ad0-412d-b3e1-a916d231a6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_df = (\n",
    "    spark.read.table(f\"{CATALOG}.{SCHEMA}.{silver_table_name}\")\n",
    "    .withColumn(\"chunks\", markdown_chunker(col(\"content\")))\n",
    "    .select(\n",
    "        \"file_name\",\n",
    "        func.explode(\"chunks\").alias(\"chunk_text\"),\n",
    "        func.md5(col(\"chunk_text\")).alias(\"chunk_id\"),\n",
    "    )\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{chunks_table_name}\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4490acb-f388-40d8-add2-ce945feb1607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of chunks\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{chunks_table_name}\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d91f2ef-a4eb-46fe-bbbc-ac665c48cae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure we have all PDF's\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{chunks_table_name}\").select(\n",
    "    \"file_name\"\n",
    ").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95cde15-1a79-4a13-afa0-4602388b5f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{chunks_table_name}\").limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a8543e1-921e-472b-b3a7-30802faca522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Generate a synthetic question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459e1809-385f-4962-a2ad-cd404d1d2add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Load the chunks\n",
    "chunks = spark.read.table(f\"{CATALOG}.{SCHEMA}.{chunks_table_name}\")\n",
    "\n",
    "# Use the ai_query SQL function to generate a question for each chunk in the table.\n",
    "chunks_with_question = chunks.withColumn(\n",
    "    \"ancher\",\n",
    "    expr(\n",
    "        \"\"\"ai_query(\n",
    "            'databricks-meta-llama-3-3-70b-instruct',\n",
    "            CONCAT(\n",
    "                'You are a question-answer pair generator. Based on the provided chunk context, create a specific, detailed question that can be fully answered using the information in the chunks. **Requirements:** 1. Carefully read the provided chunks 2. Generate a SPECIFIC question that targets key information from one or more chunks - avoid generic questions like What is the main topic? 3. Try to create a question that requires synthesizing information from multiple chunks, but only if the chunks are related 4. Only generate a single question, no and does it .. 5. Answer the question comprehensively using ONLY the information available in the chunks, but do NOT mention the chunks 6. If the chunks don''t contain sufficient information to answer, assign a score of 0 7. Provide detailed, informative answers of at least 3 sentences 8. Focus on factual content, processes, methods, or specific findings mentioned in the text A key part is that the questions should mimic a real question that a user would ask about the topics in the chunks, while the user was not aware of the chunk''s existence **Example:** Context: The study evaluated three machine learning models for sentiment analysis. Model A achieved 85 percent accuracy, Model B reached 92 percent accuracy, and Model C obtained 78 percent accuracy on the test dataset. Good Question: Which machine learning model performed best in the sentiment analysis evaluation and what were the accuracy scores of all three models? Good Answer: Model B performed best in the sentiment analysis evaluation, achieving the highest accuracy score of 92 percent on the test dataset. Model A achieved 85 percent accuracy, while Model C obtained the lowest score of 78 percent accuracy. The study compared these three models to determine their relative performance on sentiment analysis tasks. ONLY RETURN THE QUESTION',\n",
    "                chunk_text\n",
    "            ),\n",
    "            named_struct('max_tokens', 1200, 'temperature', 0.5)\n",
    "        )\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Overwrite the chunks table\n",
    "chunks_with_question.write.mode(\"overwrite\").saveAsTable(\n",
    "    f\"{CATALOG}.{SCHEMA}.{chunks_with_questions_table_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ca0e3a-9825-47cd-9788-1b154f2db06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks_with_question.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeecab93-2396-41c9-ac60-7dd4eb835426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Prepare the data for the finetuning\n",
    "\n",
    "We need a specific data structure for the sentence transformer package, this section convert the spark DF into the required format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1400fb57-c7ee-4d8d-974a-4044af0d63c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "chunks_with_question = spark.read.table(\n",
    "    f\"{CATALOG}.{SCHEMA}.{chunks_with_questions_table_name}\"\n",
    ")\n",
    "\n",
    "# Convert SparkDF to PandasDF; make initial conversion step with `from_dict` possible.\n",
    "df = chunks_with_question.toPandas()\n",
    "df = df[[\"ancher\", \"chunk_text\", \"chunk_id\"]]\n",
    "df.columns = [\"anchor\", \"positive\", \"global_chunk_id\"]\n",
    "\n",
    "# Convert from PandasDF to Dataset\n",
    "dataset = Dataset.from_dict(df)\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Use in-memory HuggingFace Datasets objects directly, avoid saving/loading from disk\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5428b627-316d-465e-bd0b-5ecec64807b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Might be able to simplify this section:\n",
    "# https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss\n",
    "\n",
    "# Combine train and test datasets into a single corpus\n",
    "# This ensures we have all possible text chunks available for retrieval evaluation\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert datasets into dictionary format required by the InformationRetrievalEvaluator\n",
    "# corpus: maps corpus IDs to their text chunks (documents)\n",
    "# Format: {corpus_id: text_chunk}\n",
    "corpus = dict(zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"]))\n",
    "\n",
    "# queries: maps query IDs to their questions\n",
    "# Format: {query_id: question_text}\n",
    "queries = dict(zip(test_dataset[\"id\"], test_dataset[\"anchor\"]))\n",
    "\n",
    "# Create a mapping between queries and their relevant documents\n",
    "# This tells the evaluator which documents are correct matches for each query\n",
    "relevant_docs = {}\n",
    "for q_id, global_chunk_id in zip(test_dataset[\"id\"], test_dataset[\"global_chunk_id\"]):\n",
    "    # Initialize empty list for each query if not already present\n",
    "    if q_id not in relevant_docs:\n",
    "        relevant_docs[q_id] = []\n",
    "\n",
    "    # Find all corpus entries that share the same global_chunk_id\n",
    "    # This handles cases where multiple questions can refer to the same text chunk\n",
    "    matching_corpus_ids = [\n",
    "        cid\n",
    "        for cid, chunk in zip(corpus_dataset[\"id\"], corpus_dataset[\"global_chunk_id\"])\n",
    "        if chunk == global_chunk_id\n",
    "    ]\n",
    "    # Add the matching corpus IDs to the relevant documents for this query\n",
    "    relevant_docs[q_id].extend(matching_corpus_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa62efcf-c5b1-4c65-a59f-9804d131a0bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Evaluate the retrieval performance with `modernbert-embed-base`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bd58d8-1299-4b9a-b11a-c3dde6805299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerModelCardData,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerTrainer,\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f253635-b7f5-4179-8022-79cbeb8dfe88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions of interest\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]  # Important: large to small\n",
    "\n",
    "# Create empty list to hold evaluators\n",
    "matryoshka_evaluators = []\n",
    "\n",
    "# Create an evaluator for each above dimension\n",
    "for dim in matryoshka_dimensions:\n",
    "    # Define the evaluator\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to the respective dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    # Add to list\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "# Create a sequential evaluator\n",
    "# Able to run all our dimension specific InformationRetrievalEvaluators sequentially.\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ecf13dd-55fc-4ece-8360-ce9242d76a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading via SentenceTransformer\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3f11e8-0fa1-4233-bcc0-33678971cc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with mlflow.start_run(run_name=\"base-model\"):\n",
    "    input_example = [\"Sample domain-specific text\"]\n",
    "    output_example = model.encode(input_example)\n",
    "    signature = mlflow.models.infer_signature(\n",
    "        model_input=input_example,\n",
    "        model_output=output_example,\n",
    "    )\n",
    "\n",
    "    model_info = mlflow.sentence_transformers.log_model(\n",
    "        model=model,\n",
    "        artifact_path=\"model\",\n",
    "        # input_example=input_example,\n",
    "        # output_example=output_example,\n",
    "        signature=signature,\n",
    "        task=\"llm/v1/embeddings\",\n",
    "    )\n",
    "    base_results = evaluator(model)\n",
    "\n",
    "    for key, value in base_results.items():\n",
    "        mlflow.log_metric(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e1c1ad-fff2-4821-a770-151a2ca35dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Finetune the model using our chunks and synthetic questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3610d0-53a9-47f5-b5e3-785a9bf50822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load model with SDPA for using Flash Attention 2\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"ModernBERT Embed base test with Matryoshka\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de7e6f3-b235-4040-9cf0-96c722243323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initial Loss\n",
    "base_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Matryoshka Loss Wrapper\n",
    "train_loss = MatryoshkaLoss(model, base_loss, matryoshka_dims=matryoshka_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4493b0-093a-4abb-ab08-ab8febaf6347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with mlflow.start_run(run_name=\"fine_tuning_experiment\"):\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    checkpoint_location = f\"{pdf_volume}/{run_id}\"\n",
    "    os.makedirs(checkpoint_location, exist_ok=True)\n",
    "\n",
    "    args = SentenceTransformerTrainingArguments(\n",
    "        output_dir=checkpoint_location,\n",
    "        num_train_epochs=1,  # Reduced from 4 - sufficient for test\n",
    "        per_device_train_batch_size=16,  # Reduced from 32\n",
    "        gradient_accumulation_steps=2,  # Reduced from 16 - effective batch size of 32\n",
    "        per_device_eval_batch_size=32,  # Increased for faster eval\n",
    "        warmup_ratio=0.05,  # Reduced warmup for shorter training\n",
    "        learning_rate=2e-5,  # Keep same - good starting point\n",
    "        lr_scheduler_type=\"linear\",  # Linear decay simpler for short training\n",
    "        optim=\"adamw_torch_fused\",  # Keep fused optimizer\n",
    "        tf32=True,  # Keep for speed\n",
    "        bf16=True,  # Keep for memory efficiency\n",
    "        batch_sampler=BatchSamplers.NO_DUPLICATES,  # Keep for ranking loss\n",
    "        eval_strategy=\"epoch\",  # Keep - only 1 eval now\n",
    "        save_strategy=\"epoch\",  # Keep\n",
    "        logging_steps=5,  # Reduced from 10 - see progress faster\n",
    "        save_total_limit=1,  # Reduced from 3 - save space\n",
    "        load_best_model_at_end=True,  # Disable for test run\n",
    "        metric_for_best_model=\"eval_dim_768_cosine_ndcg@10\",  # Keep same metric\n",
    "        report_to=\"none\",  # Keep disabled\n",
    "    )\n",
    "\n",
    "    # lot the SentenceTransformerTrainingArguments to mlflow\n",
    "    mlflow.log_params(args.to_dict())\n",
    "\n",
    "    # Initialise the trainer\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset.select_columns([\"anchor\", \"positive\"]),\n",
    "        loss=train_loss,\n",
    "        evaluator=evaluator,\n",
    "    )\n",
    "\n",
    "    # Run the training\n",
    "    trainer.train()\n",
    "\n",
    "    # Log metrics:\n",
    "    for entry in trainer.state.log_history:\n",
    "        step = entry.get(\"step\", 0)\n",
    "        for key, value in entry.items():\n",
    "            if isinstance(value, (int, float)) and key != \"step\":\n",
    "                mlflow.log_metric(key.replace(\"eval_\", \"\"), value, step=step)\n",
    "\n",
    "    # Log the fine-tuned model\n",
    "\n",
    "    best_model = trainer.model\n",
    "    input_example = [\"Sample domain-specific text\"]\n",
    "    output_example = best_model.encode(input_example)\n",
    "    signature = mlflow.models.infer_signature(\n",
    "        model_input=input_example,\n",
    "        model_output=output_example,\n",
    "    )\n",
    "\n",
    "    model_info = mlflow.sentence_transformers.log_model(\n",
    "        model=best_model,\n",
    "        artifact_path=\"model\",\n",
    "        # input_example=input_example,\n",
    "        # output_example=output_example,\n",
    "        signature=signature,\n",
    "        task=\"llm/v1/embeddings\",\n",
    "        registered_model_name=f\"{CATALOG}.{SCHEMA}.{finetune_model_id}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59663b3a-b425-4836-a913-6a03fdb9c132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Evaluate the retrieval performance on our new model\n",
    "\n",
    "In order to evaluate our finetuned embeddings model, we can look at various metrics. Each of these addresses different aspects of the retrievel, and depending on your use case you need to select most representitive metric to optimize.\n",
    "\n",
    "**Result Set Composition**\n",
    "\n",
    "- Accuracy@k: Did we find at least one relevant document in top-k results?\n",
    "  - `(queries with ≥1 relevant doc in top k) / (total queries)`\n",
    "- Precision@k: What fraction of retrieved documents are relevant?\n",
    "  - `(relevant docs in top k) / k`\n",
    "- Recall@k: What fraction of all relevant documents did we find?\n",
    "  - `(relevant docs in top k) / (total relevant docs)`\n",
    "- F1@k: Balanced measure of precision and recall\n",
    "  - `2 * (Precision@k * Recall@k) / (Precision@k + Recall@k)`\n",
    "\n",
    "**Ranking Quality & Position**\n",
    "\n",
    "- NDCG@k: How well are relevant documents ranked? (higher positions = better)\n",
    "  - `DCG@k / IDCG@k` where `DCG@k = Σ(i=1 to k) rel_i / log2(i + 2)`\n",
    "- MRR@k: How quickly do we find the first relevant result?\n",
    "  - `(1/|Q|) Σ(i=1 to |Q|) 1/rank_i`\n",
    "- MAP@k: Comprehensive ranking assessment across all relevant documents\n",
    "  - `(1/|Q|) Σ(q=1 to |Q|) AP@k(q)` where `AP@k = (1/min(k, R)) Σ(r=1 to k) (P@r * rel(r))`\n",
    "\n",
    "The definitions and explanations of the metrics are provided sourced from this [notebook](https://github.com/ALucek/ft-modernbert-domain/blob/main/FT_Embedding_Models_on_Domain_Specific_Data.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf04e0fa-e931-427f-b423-d27651fb9632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.1 Extract metrics from mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4858824c-eb32-497d-a878-15b2a22c7abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_model_rn = mlflow.get_run(\"insert your run id\")\n",
    "base_model_results = pd.DataFrame([base_model_rn.data.metrics])\n",
    "\n",
    "custom_model_run = mlflow.get_run(\"insert your run id\")\n",
    "custom_model_results = pd.DataFrame([custom_model_run.data.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "27cced0f-59cf-43f7-96f9-b971bb94d2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_model_results_stacked = base_model_results.stack().reset_index()[[\"level_1\", 0]]\n",
    "base_model_results_stacked.columns = [\"metric\", \"score\"]\n",
    "base_model_results_stacked_clean = base_model_results_stacked[\"metric\"].str.split(\n",
    "    \"_|@\", expand=True\n",
    ")[[1, 3, 4]]\n",
    "base_model_results_stacked_clean.columns = [\"dimension\", \"metric\", \"at_k\"]\n",
    "base_model_results_stacked_clean[\"score\"] = base_model_results_stacked[\"score\"]\n",
    "base_model_results_stacked_clean = base_model_results_stacked_clean[\n",
    "    base_model_results_stacked_clean.metric.notnull()\n",
    "]\n",
    "base_model_results_stacked_clean[\"model\"] = model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1dbdda99-06ce-410c-bdac-a1a2e14c63aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_model_results_stacked = custom_model_results.stack().reset_index()[\n",
    "    [\"level_1\", 0]\n",
    "]\n",
    "custom_model_results_stacked.columns = [\"metric\", \"score\"]\n",
    "custom_model_results_stacked_clean = custom_model_results_stacked[\"metric\"].str.split(\n",
    "    \"_|@\", expand=True\n",
    ")[[1, 3, 4]]\n",
    "custom_model_results_stacked_clean.columns = [\"dimension\", \"metric\", \"at_k\"]\n",
    "custom_model_results_stacked_clean[\"score\"] = custom_model_results_stacked[\"score\"]\n",
    "custom_model_results_stacked_clean = custom_model_results_stacked_clean[\n",
    "    custom_model_results_stacked_clean.metric.notnull()\n",
    "]\n",
    "custom_model_results_stacked_clean = custom_model_results_stacked_clean[\n",
    "    custom_model_results_stacked_clean.metric != \"cosine\"\n",
    "]\n",
    "custom_model_results_stacked_clean[\"model\"] = finetune_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c691dce-5e9f-4a5c-ba37-ead6aa7d1f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [base_model_results_stacked_clean, custom_model_results_stacked_clean]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d47053-4124-4737-86bc-a48285ce418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.2 Plot Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe68edb5-05b2-4975-8166-8432a601943d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "36a5e048-c2a0-4402-abd8-1bb257529dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# f1 does not come out of the box, so let's calculate it.\n",
    "f1_set = results[results.metric.isin([\"precision\", \"recall\"])]\n",
    "f1_set = f1_set.pivot_table(\n",
    "    index=[\"model\", \"dimension\", \"at_k\"], columns=\"metric\", values=\"score\"\n",
    ")\n",
    "f1_set[\"f1\"] = f1_set.apply(\n",
    "    lambda x: 2 * (x.precision * x.recall) / (x.precision + x.recall), axis=1\n",
    ")\n",
    "f1_set = (\n",
    "    f1_set.reset_index()\n",
    "    .drop(columns=[\"precision\", \"recall\"])\n",
    "    .set_index([\"dimension\", \"model\", \"at_k\"])\n",
    "    .stack()\n",
    "    .rename(\"score\")\n",
    "    .reset_index()\n",
    ")\n",
    "f1_set[\"metric\"] = \"f1\"\n",
    "\n",
    "# add back to results df\n",
    "results = pd.concat([results, f1_set])\n",
    "results = results[results.dimension != \"cosine\"]\n",
    "\n",
    "results[\"at_k\"] = results[\"at_k\"].astype(str)\n",
    "results[\"dimension\"] = results[\"dimension\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5276263b-c7df-450a-8566-a8745cbd17ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    results[results.metric.isin([\"accuracy\", \"precision\", \"recall\", \"f1\"])],\n",
    "    title=\"Result Set Composition Metrics\",\n",
    "    x=\"at_k\",\n",
    "    y=\"score\",\n",
    "    facet_col=\"dimension\",\n",
    "    facet_row=\"metric\",\n",
    "    color=\"model\",\n",
    "    barmode=\"group\",\n",
    "    height=1000,\n",
    "    category_orders={\n",
    "        \"dimension\": [\"768\", \"512\", \"256\", \"128\", \"64\"],\n",
    "        \"at_k\": [\"1\", \"3\", \"5\", \"10\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.update_yaxes(tickformat=\".0%\", dtick=0.2, range=[0, 1])\n",
    "\n",
    "# As x is interval, we need to change the type to category to avoid empty bars.\n",
    "fig.update_xaxes(type=\"category\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db8599d-4ddf-447b-8119-df072bb3c3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    results[results.metric.isin([\"mrr\", \"ndcg\"])],\n",
    "    title=\"Ranking Quality Metrics\",\n",
    "    x=\"at_k\",\n",
    "    y=\"score\",\n",
    "    facet_col=\"dimension\",\n",
    "    facet_row=\"metric\",\n",
    "    color=\"model\",\n",
    "    barmode=\"group\",\n",
    "    height=1000,\n",
    "    category_orders={\n",
    "        \"dimension\": [\"768\", \"512\", \"256\", \"128\", \"64\"],\n",
    "    },\n",
    "    text=\"score\",\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    tickformat=\".0%\", dtick=0.1, range=[0, 1], showticklabels=False, title=None\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate=\"%{text:.0%}\", textposition=\"outside\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Serve Model through a Serving Endpoint\n",
    "\n",
    "Because the model is registered in Unity Catalog, with the task `llm/v1/embeddings` it can be directly deployed to a serving endpoint.\n",
    "\n",
    "# ![Model Finetune](model-finetune.png)\n",
    "\n",
    "# ![Model Serving](serving-finetune.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "finetuning-embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
